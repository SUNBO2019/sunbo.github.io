# 持续更新中......

### 经典机器学习（Classic Machine Learning）
* <a href="https://github.com/SUNBO2019/sunbo2019.github.io/blob/master/GDOA_01.pdf" >机器学习中的优化器（01）</a><br/>
* 机器学习中的优化器（02）(草稿中...) <br/>
* 机器学习中的优化器（03）(草稿中...) <br/>
* 朴素贝叶斯 <br/>
* <a href="https://github.com/SUNBO2019/sunbo2019.github.io/blob/master/laplace_smoothing_20190716.pdf" >拉普拉斯平滑（Laplace Smoothing）</a><br/>
* 决策树 <br/>
* 集成学习 <br/>
* 梯度下降 <br/>
* 线性回归 <br/>
* 对数几率回归及正则化 <br/>
* PCA <br/>
* <a href="https://github.com/SUNBO2019/sunbo2019.github.io/blob/master/Optimization.pdf" target="view_window">优化（Optimization）</a><br/>
* <a href="https://github.com/SUNBO2019/sunbo2019.github.io/blob/master/EM.pdf">EM算法</a><br/>
* <a href="https://github.com/SUNBO2019/sunbo2019.github.io/blob/master/SVM.pdf">支持向量机（SVM）</a><br/>
* <a href="https://github.com/SUNBO2019/sunbo2019.github.io/blob/master/SVM_v2.0.pdf">支持向量机（SVM）_v2.0(new)</a><br/>
* <a href="https://github.com/SUNBO2019/sunbo2019.github.io/blob/master/GMM.pdf">高斯混合模型（GMM）</a><br/>
* <a href="https://github.com/SUNBO2019/sunbo2019.github.io/blob/master/HMM.pdf" target="view_window">隐马尔可夫模型（HMM）</a><br/>
* 高斯过程 <br/>
* K-近邻算法（KNN） <br/>
* K-Means算法 <br/>

### 深度学习（Deep Learning）

* <a href="https://github.com/SUNBO2019/sunbo2019.github.io/blob/master/MLP.pdf">多层感知机（MLP）</a><br/>
* <a href="https://github.com/SUNBO2019/sunbo2019.github.io/blob/master/NNLM.pdf">神经网络语言模型（NNLM）</a><br/>
* <a href="https://github.com/SUNBO2019/sunbo2019.github.io/blob/master/Activation_Function.pdf">激活函数</a><br/>
* 损失函数 <br/>
* 卷积神经网络（Convolutional Neural Networks, CNN）<br/>
* <a href="https://github.com/SUNBO2019/sunbo2019.github.io/blob/master/RNN.pdf">循环神经网络（Recurrent Neural Network, RNN)</a><br/>
* <a href="https://github.com/SUNBO2019/sunbo2019.github.io/blob/master/GRU.pdf">门控循环单元（Gated Recurrent Unit, GRU)</a><br/>

### 自然语言处理（Natural Language Processing）
* 长短期记忆网络（Long Short-Term Memory, LSTM）（草稿中 ... ）<br/>
* Seq2Seq（Sequence to Sequence）（草稿中 ... ）<br/>
* <a href="https://github.com/SUNBO2019/sunbo2019.github.io/blob/master/Attention_v2.pdf">Attention机制</a><br/>
* Transformer模型<br/>
* BERT(Bidirectional Encoder Representations from Transformers) <br/>
* 文本相似度 （草稿中 ...）<br/>
* 排序模型（Learning to Rank,  L2R） （草稿中 ...）<br/>
* 主题模型（Latent Dirichlet Allocation, LDA 隐含狄利克雷分布）<br/>

